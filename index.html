<!DOCTYPE html>
<html>
  <head>
    <title>SERENA</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="helper.js" defer></script>
    <style>
      td {
        vertical-align: middle;
      }
      audio {
        width: 20vw;
        min-width: 100px;
        max-width: 250px;
      }
    </style>
  </head>
  <body>
    <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
      <div class="text-center">
        <h1>SERENA</h1>
        <h3 style="margin-bottom: 20px;">Sensado, monitoreo y estudio del entorno sonoro para  <br> la evaluación del impacto antropogénico en Antártida </h3>
      </div>
      <p>
        <b>Abstract.</b> We introduce AudioLM, a framework for high-quality
        audio generation with long-term consistency. AudioLM maps the input
        audio to a sequence of discrete tokens and casts audio generation as a
        language modeling task in this representation space. We show how
        existing audio tokenizers provide different trade-offs between
        reconstruction quality and long-term structure, and we propose a hybrid
        tokenization scheme to achieve both objectives. Namely, we leverage the
        discretized activations of a masked language model pre-trained on audio
        to capture long-term structure and the discrete codes produced by a
        neural audio codec to achieve high-quality synthesis. By training on
        large corpora of raw audio waveforms, AudioLM learns to generate natural
        and coherent continuations given short prompts. When trained on speech,
        and without any transcript or annotation, AudioLM generates
        syntactically and semantically plausible speech continuations while also
        maintaining speaker identity and prosody for unseen speakers.
        Furthermore, we demonstrate how our approach extends beyond speech by
        generating coherent piano music continuations, despite being trained
        without any symbolic representation of music.
      </p>
    </div>

    
    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Unconditional generation</h3>

      <p class="mb-0">
        <br />
        The unconditional generation performs sampling without using prompts. In
        that case, every sequence varies in speaker identity, linguistic
        content, and recording conditions.
      </p>

      <div class="container pt-3 table-responsive">
        <table
          class="table table-striped table-hover"
          id="unconditional-speech-table"
        >
          <thead>
            <tr>
              <th>Samples from unconditional generation</th>
            </tr>
          </thead>
          <tbody></tbody>
        </table>
      </div>
    </div>

    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Generation without semantic tokens</h3>

      <p class="mb-0">
        <br />
        To illustrate that the semantic tokens are crucial for generating
        coherent linguistic content, we train the language model on the acoustic
        tokens only. While the generated continuations of the 4-second prompts
        maintain speaker identity, the linguistic content is inconsistent, and
        often akin to babbling.
      </p>

      <div class="container pt-3 table-responsive">
        <table class="table table-striped table-hover" id="semantic-only-table">
          <thead>
            <tr>
              <th>
                Continuations with a language model trained on the acoustic
                tokens only (without semantic tokens)
              </th>
            </tr>
          </thead>
          <tbody></tbody>
        </table>
      </div>
    </div>
  </body>
</html>
